### Exercise: Web Crawler

идея и заготовка взята из ["A Tour of Go"](https://go.dev/tour/concurrency/10v)

Дано: код Веб-сканера на go.
Сделать: Измените функцию Crawl, чтобы она получала URL-адреса параллельно, без повторной выборки одного и того же URL-адреса.

`go build -o myapp2 stupidCrawler/stupidCrawler.go`


### Отчет по проекту: Реализация однопоточного и конкурентного веб-сканеров на Go

#### Введение

В рамках проекта была поставлена задача разработать два веб-сканера на языке Go: один с использованием однопоточного подхода и другой с параллельной обработкой запросов. Основной целью было изучить разницу в производительности между этими подходами при извлечении URL-адресов с веб-страниц, а также обеспечить эффективное использование памяти и предотвратить повторное сканирование одних и тех же URL-адресов.

#### Описание задачи

Задача заключалась в реализации веб-сканера, который:

1. Проходит по указанному URL-адресу.
2. Извлекает все ссылки (URL-адреса) на этой странице.
3. Рекурсивно сканирует эти ссылки, избегая повторной выборки одних и тех же URL-адресов.

#### Реализованные решения

1. **Однопоточный веб-сканер**: В этом варианте сканер последовательно проходит каждый URL-адрес, извлекая с каждой страницы новые ссылки и посещая их одну за другой. При этом для каждого URL выполняется запрос к серверу, затем извлекаются ссылки и снова выполняются запросы для каждой найденной ссылки. Этот подход работает последовательно и не использует параллелизм, что делает его медленным при большом числе URL-адресов.
    
2. **Конкурентный веб-сканер**: Для увеличения производительности была использована параллельная обработка запросов с помощью горутин (goroutines). В этом варианте для каждого нового URL-адреса, найденного на странице, запускается новая горутина, что позволяет одновременно обрабатывать несколько URL-адресов. Это значительно увеличивает скорость работы, особенно при большом количестве URL-адресов, так как каждый запрос выполняется параллельно с другими.

